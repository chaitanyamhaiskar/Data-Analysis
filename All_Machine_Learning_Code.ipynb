{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayJetCUMGqnA"
      },
      "outputs": [],
      "source": [
        "# Linear Regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "reg_model = LinearRegression()\n",
        "reg_model.fit(X_train, y_train)\n",
        "y_pred= reg_model.predict(X_test)\n",
        "x_pred= reg_model.predict(X_train)\n",
        "reg_model_diff = pd.DataFrame({'Actual value': y_test, 'Predicted value': y_pred})\n",
        "reg_model_diff\n",
        "\n",
        "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
        "mse = metrics.mean_squared_error(y_test, y_pred)\n",
        "r2 = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print('Mean Absolute Error:', mae)\n",
        "print('Mean Square Error:', mse)\n",
        "print('Root Mean Square Error:', r2)\n",
        "\n",
        "print(\"R squared: {}\".format(r2_score(y_true=y_train,y_pred=y_pred)))\n",
        "\n",
        "# Assumption\n",
        "# 1. Mean of residual - mean of the residuals should be zero\n",
        "residuals = y_train.values-y_pred\n",
        "mean_residuals = np.mean(residuals)\n",
        "print(\"Mean of Residuals {}\".format(mean_residuals))\n",
        "\n",
        "# 2.Check for Homoscedasticity\n",
        "p = sns.scatterplot(y_pred,residuals)\n",
        "plt.xlabel('y_pred/predicted values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.ylim(-10,10)\n",
        "plt.xlim(0,26)\n",
        "p = sns.lineplot([0,26],[0,0],color='blue')\n",
        "p = plt.title('Residuals vs fitted values plot for homoscedasticity check')\n",
        "\n",
        "# 3. Check for Normality of error terms/residuals\n",
        "p = sns.distplot(residuals,kde=True)\n",
        "p = plt.title('Normality of error terms/residuals')\n",
        "\n",
        "# 4. No autocorrelation of residuals - There should not be autocorrelation in the data so the error terms should not form any pattern\n",
        "plt.figure(figsize=(10,5))\n",
        "p = sns.lineplot(y_pred,residuals,marker='o',color='blue')\n",
        "plt.xlabel('y_pred/predicted values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.ylim(-10,10)\n",
        "plt.xlim(0,26)\n",
        "p = sns.lineplot([0,26],[0,0],color='red')\n",
        "p = plt.title('Residuals vs fitted values plot for autocorrelation check')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Classifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "dec_tree = DecisionTreeRegressor(random_state=0)\n",
        "dec_tree.fit(X_train,y_train)\n",
        "dec_tree_y_pred = dec_tree.predict(X_train)\n",
        "print(\"Accuracy: {}\".format(dec_tree.score(X_train,y_train)))\n",
        "print(\"R squared: {}\".format(r2_score(y_true=y_train,y_pred=dec_tree_y_pred)))\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "tree_clf = DecisionTreeClassifier(random_state=42)\n",
        "tree_clf.fit(X_train, y_train)\n",
        "y_pred = tree_clf.fit(x_test)\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "from sklearn import tree\n",
        "plt.figure(figsize = (20, 10))\n",
        "tree.plot_tree(dt, feature_names = X_train.columns.values);\n",
        "# plt.savefig('sampleTree.jpeg)\n",
        "\n",
        "\n",
        "params = {\"criterion\":(\"gini\", \"entropy\"),\n",
        "          \"splitter\":(\"best\", \"random\"),\n",
        "          \"max_depth\":(list(range(1, 20))),\n",
        "          \"min_samples_split\":[2, 3, 4],\n",
        "          \"min_samples_leaf\":list(range(1, 20))\n",
        "          }\n",
        "\n",
        "\n",
        "import graphviz\n",
        "from six import StringIO\n",
        "from IPython.display import Image\n",
        "import pydotplus\n",
        "import pydot\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "dot_data = StringIO()\n",
        "export_graphviz(dt, out_file = dot_data,\n",
        "                filled = True, rounded = True,\n",
        "                special_characters = True)\n",
        "graph = pydotplus.graphviz.graph_from_dot_data(dot_data.getvalue())\n",
        "\n",
        "Image(graph.create_png())\n",
        "\n"
      ],
      "metadata": {
        "id": "4SL1Cq5JJD6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomForest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf_tree = RandomForestRegressor(random_state=0)\n",
        "rf_tree.fit(X_train,y_train)\n",
        "rf_tree_y_pred = rf_tree.predict(X_train)\n",
        "print(\"Accuracy: {}\".format(rf_tree.score(X_train,y_train)))\n",
        "print(\"R squared: {}\".format(r2_score(y_true=y_train,y_pred=rf_tree_y_pred)))\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "n_estimators = [500, 900, 1100, 1500]\n",
        "max_features = ['auto', 'sqrt']\n",
        "max_depth = [2, 3, 5, 10, 15, None]\n",
        "min_samples_split = [2, 5, 10]\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "\n",
        "params_grid = {\n",
        "    'n_estimators': n_estimators,\n",
        "    'max_features': max_features,\n",
        "    'max_depth': max_depth,\n",
        "    'min_samples_split': min_samples_split,\n",
        "    'min_samples_leaf': min_samples_leaf"
      ],
      "metadata": {
        "id": "9FpGRbEwJH2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "svr = SVR()\n",
        "svr.fit(X_train,y_train)\n",
        "svr_y_pred = svr.predict(X_train)\n",
        "print(\"Accuracy: {}\".format(svr.score(X_train,y_train)))\n",
        "print(\"R squared: {}\".format(r2_score(y_true=y_train,y_pred=svr_y_pred)))\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_clf = SVC(kernel='rbf', gamma=0.1, C=1.0)\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "params = {\"C\":(0.1, 0.5, 1, 2, 5, 10, 20),\n",
        "          \"gamma\":(0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1),\n",
        "          \"kernel\":('linear', 'poly', 'rbf')}"
      ],
      "metadata": {
        "id": "u8-vG6L1JK5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_clf = LogisticRegression(solver='liblinear')\n",
        "lr_clf.fit(X_train, y_train)\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_predlr)\n",
        "auprc = auc(recall, precision)\n",
        "\n",
        "# Plotting AUPRC\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()\n",
        "\n",
        "print('AUPRC = ', auprc)\n"
      ],
      "metadata": {
        "id": "kLhBokf4J9xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_clf = KNeighborsClassifier()\n",
        "knn_clf.fit(X_train, y_train)\n",
        "\n",
        "# HyperTunning\n",
        "train_score = []\n",
        "test_score = []\n",
        "neighbors = range(1, 30)\n",
        "\n",
        "for k in neighbors:\n",
        "    model = KNeighborsClassifier(n_neighbors=k)\n",
        "    model.fit(X_train, y_train)\n",
        "    train_score.append(accuracy_score(y_train, model.predict(X_train)))\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(10, 7))\n",
        "\n",
        "  plt.plot(neighbors, train_score, label=\"Train score\")\n",
        "  # plt.plot(neighbors, test_score, label=\"Test score\")\n",
        "  plt.xticks(np.arange(1, 21, 1))\n",
        "  plt.xlabel(\"Number of neighbors\")\n",
        "  plt.ylabel(\"Model score\")\n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "6i3EhaP_KEU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression - Ridge and Lasso\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "ridge = Ridge(alpha=.3) #coefficients are prevented to become too big by this alpha value\n",
        "ridge.fit(X_train,y_train)\n",
        "for i,col in enumerate(X_train.columns):\n",
        "    print (\"Ridge model coefficients for {} is {}:\".format(col,ridge.coef_[0][i]))\n",
        "\n",
        "print(ridge.score(X_train, y_train))\n",
        "print(ridge.score(X_test, y_test))\n",
        "\n",
        "\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_train,y_train)\n",
        "for i,col in enumerate(X_train):\n",
        "    print (\"Lasso model coefficients for {} is {}:\".format(col,lasso.coef_[i]))\n",
        "\n",
        "print(lasso.score(X_train, y_train))\n",
        "print(lasso.score(X_test, y_test))\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train2, y_train)\n",
        "y_pred = lr.predict(x_test2)\n",
        "y_predr1 = lr.predict(x_train2)\n",
        "# Transform data using fitted StandardScaler\n",
        "\n",
        "x_test_transformed = pd.DataFrame(sc.transform(x_test), index = x_test.index, columns = x_test.columns)\n",
        "\n",
        "# Retrieve coefficient from LogisticRegression model\n",
        "LogisticCoeff = pd.concat([pd.DataFrame(x_test_transformed.columns), pd.DataFrame(np.transpose(lr.coef_))], axis=1)\n",
        "LogisticCoeff.columns = ['Variable', 'Coefficient']\n",
        "LogisticCoeff\n",
        "\n",
        "\n",
        "lr = LogisticRegression(penalty = 'l1', solver = 'liblinear')\n",
        "lr.fit(x_train2, y_train)\n",
        "y_predL1 = lr.predict(x_test2)\n",
        "y_predli = lr.predict(x_train2)\n",
        "\n",
        "x_test_transformed = pd.DataFrame(sc.transform(x_test), index = x_test.index, columns = x_test.columns)\n",
        "LassoCoeff = pd.concat([pd.DataFrame(x_test_transformed.columns), pd.DataFrame(np.transpose(lr.coef_))], axis=1)\n",
        "\n",
        "LassoCoeff.columns = ['Variable', 'Coefficient']\n",
        "LassoCoeff\n"
      ],
      "metadata": {
        "id": "0IEQM8-CLy8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "ks = range(1,6)\n",
        "inertias = []\n",
        "\n",
        "for k in ks:\n",
        "    model = KMeans(n_clusters = k) # Create a KMean instance with K clusters: model\n",
        "    model.fit(samples) # Fit models to samples\n",
        "    inertias.append(model.inertia_) # Append the inertia to list of inertias\n",
        "\n",
        "inertias # Inertia is an error, also known as wcss\n",
        "plt.plot(ks, inertias, '-o')\n",
        "plt.xlabel('No. of Clusters, k');\n",
        "plt.ylabel('Inertia');\n",
        "plt.xticks(ks);\n",
        "plt.title('Elbow Curve');\n",
        "\n",
        "model = KMeans(n_clusters = 3)\n",
        "labels = model.fit_predict(samples)\n",
        "labels\n",
        "\n",
        "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
        "ct = pd.crosstab(df['labels'], df['varieties'])\n",
        "ct\n",
        "\n",
        "\n",
        "import scipy.cluster.hierarchy as shc\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.title(\"Customer Dendograms\")\n",
        "dend = shc.dendrogram(shc.linkage(data, method='ward'))\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\n",
        "labels_=cluster.fit_predict(data)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(data[:,0], data[:,1], c=cluster.labels_, cmap='rainbow')"
      ],
      "metadata": {
        "id": "7Gs-fk_kMl0r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}